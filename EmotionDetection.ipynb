{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A50IBXskizNZ","executionInfo":{"status":"ok","timestamp":1749451769057,"user_tz":-330,"elapsed":6765,"user":{"displayName":"S Sanjeevarao","userId":"15061285982683468143"}},"outputId":"d2e0de9b-cf17-4486-ab96-f5931364171d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ensuring OpenCV, matplotlib, Pillow, and fer are installed...\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (10.2.0)\n","Requirement already satisfied: fer in /usr/local/lib/python3.11/dist-packages (22.5.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from fer) (4.11.0.86)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fer) (3.8.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fer) (2.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from fer) (2.32.3)\n","Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.11/dist-packages (from fer) (2.6.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from fer) (4.67.1)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from fer) (1.0.3)\n","Requirement already satisfied: ffmpeg==1.4 in /usr/local/lib/python3.11/dist-packages (from fer) (1.4)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.0.0->fer) (1.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.0.0->fer) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.0.0->fer) (0.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.0.0->fer) (3.13.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.0.0->fer) (0.16.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.0.0->fer) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch->fer) (2.2.2)\n","Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch->fer) (0.17.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->fer) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->fer) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->fer) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->fer) (2025.4.26)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy->fer) (4.4.2)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->fer) (0.1.12)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy->fer) (2.37.0)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->fer) (0.6.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fer) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fer) (2025.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (4.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.5.82)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.0.0->fer) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.0.0->fer) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->fer) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.3.0)\n","\n","Installation complete. Proceed to next cell.\n"]}],"source":["# Cell 1: Install Required Libraries\n","print(\"Ensuring OpenCV, matplotlib, Pillow, and fer are installed...\")\n","\n","!pip install opencv-python matplotlib Pillow fer\n","\n","print(\"\\nInstallation complete. Proceed to next cell.\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"nxOjWeYri8Mq","outputId":"2a4ef93b-2843-476d-affb-b7b172785174","executionInfo":{"status":"error","timestamp":1749451910977,"user_tz":-330,"elapsed":492,"user":{"displayName":"S Sanjeevarao","userId":"15061285982683468143"}}},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-8fc5649286b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Line 10: Import the FER (Face Emotion Recognition) detector class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# This is the specific tool that will help us analyze emotions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Importing libraries and preparing...\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Just a helpful message to let us know what's happening.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fer/classes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"]}],"source":["# Cell 2: Import Libraries, Download Haar Cascade, and Load Image\n","\n","# Line 1: Import the OpenCV library\n","# This is our main toolkit for computer vision tasks, like reading images and finding faces.\n","import cv2\n","\n","# Line 2: Import the Matplotlib plotting library\n","# We'll use this to display our image and draw rectangles and text on it.\n","import matplotlib.pyplot as plt\n","\n","# Line 3: Import Matplotlib's patches module\n","# Specifically, this part of Matplotlib lets us draw shapes like rectangles on our plot.\n","import matplotlib.patches as patches\n","\n","# Line 4: Import the Pillow (PIL) library\n","# Pillow helps us work with images, especially for opening and sometimes for display.\n","from PIL import Image\n","\n","# Line 5: Import the NumPy library\n","# NumPy is essential for working with numerical data in Python, especially arrays.\n","# Images are often represented as NumPy arrays, so it's super important here.\n","import numpy as np\n","\n","# Line 6: Import the time library\n","# We'll use this to measure how long our face and emotion detection process takes.\n","import time\n","\n","# Line 7: Import the 'files' module from Google Colab\n","# This is a special Colab-specific tool that lets us easily upload files (like our image)\n","# directly from our computer into the Colab environment.\n","from google.colab import files\n","\n","# Line 8: Import the 'os' (operating system) module\n","# We use this to interact with the computer's file system, like checking if a file already exists.\n","import os\n","\n","# Line 9: Import the 'operator' module\n","# This is a handy tool we'll use later to easily find the emotion with the highest score.\n","import operator\n","\n","# Line 10: Import the FER (Face Emotion Recognition) detector class\n","# This is the specific tool that will help us analyze emotions.\n","from fer import FER\n","\n","print(\"Importing libraries and preparing...\") # Just a helpful message to let us know what's happening.\n","\n","# --- Download Haar Cascade XML file ---\n","# Line 11: Define the URL for the Haar Cascade XML file\n","# This is the web address where we can download the pre-trained model for face detection.\n","haar_cascade_url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'\n","# Line 12: Define the filename we'll save the Haar Cascade model as\n","haar_cascade_filename = 'haarcascade_frontalface_default.xml'\n","\n","print(f\"Downloading Haar Cascade file: {haar_cascade_filename}...\") # Another helpful message.\n","\n","# Line 13: Check if the Haar Cascade file already exists\n","# We use 'os.path.exists()' to see if we've downloaded this file before.\n","if not os.path.exists(haar_cascade_filename):\n","    # Line 14: If the file doesn't exist, download it using 'wget'\n","    # '!wget' is a command line tool that downloads files from the internet.\n","    # '-O' tells it to save the file with the specified filename.\n","    !wget {haar_cascade_url} -O {haar_cascade_filename}\n","    print(\"Download complete.\") # Confirmation message.\n","else:\n","    print(\"Haar Cascade file already exists.\") # Message if we don't need to download again.\n","\n","\n","# --- Upload your image ---\n","print(\"\\nUploading your image...\") # Prompt for the user to upload their image.\n","\n","# Line 15: Open the file upload dialog\n","# This command from Google Colab's 'files' module will pop up a window,\n","# allowing you to select an image file from your computer to upload to Colab.\n","uploaded = files.upload()\n","\n","# Line 16: Get the name of the uploaded file\n","# Once you upload a file, it's stored in the 'uploaded' variable. This line extracts\n","# the actual filename (e.g., 'my_picture.jpg') so we can use it.\n","image_filename = next(iter(uploaded))\n","\n","# --- Load the image ---\n","# Line 17: Read the image into our program using OpenCV\n","# 'cv2.imread()' opens the image file. OpenCV reads images as a NumPy array,\n","# but it uses a color order called BGR (Blue, Green, Red) by default.\n","img_bgr = cv2.imread(image_filename)\n","\n","# Line 18: Check if the image was loaded successfully\n","# Sometimes, if the file path is wrong or the file is corrupted, 'cv2.imread' might return 'None'.\n","# This check prevents errors later if the image didn't load.\n","if img_bgr is None:\n","    print(f\"Error: Could not load image from {image_filename}. Check file path.\")\n","else:\n","    # Line 19: Convert the image from BGR to RGB\n","    # Matplotlib, which we'll use for displaying, expects images in RGB (Red, Green, Blue) format.\n","    # This line converts it so the colors look correct when we show the image.\n","    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n","\n","    # Line 20: Get the height, width, and number of color channels of the image\n","    # The 'shape' attribute of a NumPy array (which our image is) tells us its dimensions.\n","    # We unpack it into 'height', 'width', and '_' (we don't need the channel info here).\n","    height, width, _ = img_rgb.shape\n","\n","    # Line 21: Set a base width for our plot's figure\n","    # This helps control how large the image appears when plotted.\n","    fig_width = 15\n","\n","    # Line 22: Calculate the proportional height for our plot's figure\n","    # We calculate the height to match the image's original aspect ratio (width-to-height ratio).\n","    # This prevents the image from looking stretched or squashed on the plot.\n","    fig_height = fig_width * (height / width)\n","\n","    print(f\"Image '{image_filename}' loaded. Dimensions: {width}x{height}\") # Confirmation message."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"hHgSOI62jEmb","outputId":"6bee54ad-43f9-4a58-bfac-986ffd0362f3","executionInfo":{"status":"error","timestamp":1749451888110,"user_tz":-330,"elapsed":824,"user":{"displayName":"S Sanjeevarao","userId":"15061285982683468143"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Initializing detectors and performing detection...\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'haar_cascade_filename' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-412c7c2ec083>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Think of this as loading a special \"face-finding map\" into our program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# This map tells OpenCV exactly what features to look for to spot a face.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mface_cascade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhaar_cascade_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Line 2: Initialize the FER (Face Emotion Recognition) detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'haar_cascade_filename' is not defined"]}],"source":["# Cell 3: Face Detection, Emotion Detection, and Plotting\n","\n","print(\"\\nInitializing detectors and performing detection...\")\n","\n","# Line 1: Initialize the Haar Cascade classifier for face detection\n","# Think of this as loading a special \"face-finding map\" into our program.\n","# This map tells OpenCV exactly what features to look for to spot a face.\n","face_cascade = cv2.CascadeClassifier(haar_cascade_filename)\n","\n","# Line 2: Initialize the FER (Face Emotion Recognition) detector\n","# This loads another smart tool, specifically designed to read emotions from faces.\n","# We set 'mtcnn=False' because we're already finding faces with our 'face_cascade'.\n","# This tells FER, \"Don't bother finding faces yourself; I'll give them to you!\"\n","emotion_detector = FER(mtcnn=False)\n","\n","# Line 3: Mark the starting time\n","# We're just setting a stopwatch here to see how long our entire process takes.\n","start = time.time()\n","\n","# --- 1. Face Detection using OpenCV Haar Cascade ---\n","# Line 4: Convert the image to grayscale\n","# Imagine taking a color photo and making it black and white. Why?\n","# Because our 'face_cascade' (the face-finding map) works much faster and more efficiently\n","# when it only has to worry about shades of gray, not millions of colors.\n","gray_image = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n","\n","# Line 5: Detect faces! This is the core face-finding step.\n","# We're giving our grayscale image to the 'face_cascade' and asking it to find all faces.\n","# The numbers (1.1, 5, (30, 30)) are like sensitivity settings:\n","#   - 1.1: How much to shrink the image each time it looks for faces (smaller means more thorough, but slower).\n","#   - 5: How many times a potential face region needs to be seen to be counted as a real face (helps avoid false alarms).\n","#   - (30, 30): The smallest face size (in pixels) we're willing to detect. Anything smaller is ignored.\n","# 'faces' will become a list of rectangles, each one describing a face's position (x, y, width, height).\n","faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n","\n","# --- Setup Plot ---\n","# Line 6: Prepare a place to draw our image and the results\n","# This creates a blank canvas (called a 'figure') and a drawing area ('axes') for our picture.\n","# 'figsize' sets the size of this canvas, making sure our image isn't tiny or huge.\n","fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n","# Line 7: Display the original color image on our drawing area\n","# We're putting our actual picture onto the canvas. 'aspect='auto'' helps it fit nicely.\n","ax.imshow(img_rgb, aspect='auto')\n","\n","print(\"\\n--- Detected Faces & Emotions ---\")\n","# Line 8: Keep a count of how many emotions we successfully detect\n","detected_emotions_count = 0\n","\n","# Line 9: Set a padding for extracting face images\n","# When we cut out a face to analyze its emotion, we want to grab a little extra space\n","# around it (like a border) so the emotion detector has more context. '20' pixels in this case.\n","padding_boundary = 20\n","\n","# Line 10: Loop through each detected face\n","# Now, we're going through each 'face' that our 'face_cascade' found, one by one.\n","# For each face, 'x', 'y', 'w', and 'h' tell us its top-left corner (x,y) and its width (w) and height (h).\n","for (x, y, w, h) in faces:\n","    # Line 11: Convert OpenCV coordinates to (top, right, bottom, left)\n","    # The 'face_recognition' library often uses this format, and it's also clear for drawing.\n","    # 'y' is 'top', 'x + w' is 'right', 'y + h' is 'bottom', and 'x' is 'left'.\n","    top, right, bottom, left = y, x + w, y + h, x\n","\n","    print(f\"  Face found at X: {x}, Y: {y}, Width: {w}, Height: {h}\")\n","\n","    # Line 12: Draw a red rectangle around the face\n","    # We're creating a rectangle object using the (left, top) corner, width, and height.\n","    # 'linewidth' is how thick the line is, 'edgecolor='r'' makes it red, and 'facecolor='none'' means it's just an outline.\n","    rect = patches.Rectangle((left, top), (right - left), (bottom - top),\n","                             linewidth=3, edgecolor='r', facecolor='none')\n","    # Line 13: Add the rectangle to our drawing area ('ax')\n","    ax.add_patch(rect)\n","\n","    # --- 2. Extract Face Image for Emotion Detection ---\n","    # Line 14: Calculate the starting Y-coordinate for slicing the face image\n","    # We take 'y' (top of the face) and subtract 'padding_boundary'.\n","    # 'max(0, ...)' makes sure we don't go below 0 (the top edge of the image).\n","    y_start = max(0, y - padding_boundary)\n","    # Line 15: Calculate the ending Y-coordinate for slicing the face image\n","    # We take 'y + h' (bottom of the face) and add 'padding_boundary'.\n","    # 'min(img_bgr.shape[0], ...)' makes sure we don't go beyond the image's total height.\n","    y_end = min(img_bgr.shape[0], y + h + padding_boundary)\n","    # Line 16: Calculate the starting X-coordinate for slicing the face image\n","    # Similar to y_start, but for the left edge.\n","    x_start = max(0, x - padding_boundary)\n","    # Line 17: Calculate the ending X-coordinate for slicing the face image\n","    # Similar to y_end, but for the right edge.\n","    x_end = min(img_bgr.shape[1], x + w + padding_boundary)\n","\n","    # Line 18: Slice the image to get just the face (with padding)\n","    # This creates a smaller image containing only the detected face region.\n","    # We're using 'img_bgr' here because it's the original image loaded by OpenCV.\n","    face_image_for_emotion = img_bgr[y_start:y_end, x_start:x_end]\n","\n","    # --- 3. Perform Emotion Detection ---\n","    # Line 19: Check if the sliced face image is not empty\n","    # Sometimes, if a face is at the very edge of the image, the slicing might result in an empty area.\n","    # This check prevents errors if that happens.\n","    if face_image_for_emotion.size > 0:\n","        # Line 20: Convert the sliced face image to RGB\n","        # Our 'fer' emotion detector often works best with RGB images, so we convert it.\n","        face_image_rgb = cv2.cvtColor(face_image_for_emotion, cv2.COLOR_BGR2RGB)\n","\n","        # Line 21: Detect emotions in this extracted face image\n","        # We give the small RGB face image to our 'emotion_detector'.\n","        # It processes it and gives us back information about the emotions it found.\n","        # This 'emotions_data' will be a list of dictionaries.\n","        emotions_data = emotion_detector.detect_emotions(face_image_rgb)\n","\n","        # Line 22: Check if any emotions were actually detected for this face\n","        # If 'emotions_data' is not empty, it means FER successfully found emotions.\n","        if emotions_data:\n","            # Line 23: Get the emotion scores for the first face detected in the slice\n","            # If FER found multiple faces in our small slice (unlikely but possible),\n","            # 'emotions_data[0]' gives us the first one's emotion scores (e.g., 'happy': 0.9, 'sad': 0.1).\n","            emotion_scores = emotions_data[0]['emotions']\n","            # Line 24: Find the emotion with the highest score\n","            # 'max(...)' finds the item with the biggest number (the highest score).\n","            # 'operator.itemgetter(1)' tells it to look at the *second* part of each item (the score itself).\n","            # '[0]' then picks out the emotion name (like 'happy').\n","            emotion = max(emotion_scores.items(), key=operator.itemgetter(1))[0]\n","            # Line 25: Increment our counter for detected emotions\n","            detected_emotions_count += 1\n","\n","            # Line 26: Plot the emotion text above the face\n","            # 'plt.text' draws text on our image.\n","            #   - 'x, y - 10': Places the text slightly above the top-left corner of the face.\n","            #   - 'fontsize', 'color': Self-explanatory.\n","            #   - 'bbox': Creates a nice colored box behind the text, making it easier to read.\n","            plt.text(x, y - 10, emotion, fontsize=12, color='blue',\n","                     bbox=dict(facecolor='yellow', edgecolor='blue', boxstyle='round,pad=0.3'))\n","        else:\n","            # Line 27: If no emotion was confidently detected, print a message and draw \"Unknown\"\n","            print(f\"  No emotion detected for face at X:{x}, Y:{y}.\")\n","            plt.text(x, y - 10, \"Unknown\", fontsize=10, color='gray',\n","                     bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.2'))\n","# Cell 3: Face Detection, Emotion Detection, and Plotting\n","\n","print(\"\\nInitializing detectors and performing detection...\")\n","\n","# Line 1: Initialize the Haar Cascade classifier for face detection\n","# Think of this as loading a special \"face-finding map\" into our program.\n","# This map tells OpenCV exactly what features to look for to spot a face.\n","face_cascade = cv2.CascadeClassifier(haar_cascade_filename)\n","\n","# Line 2: Initialize the FER (Face Emotion Recognition) detector\n","# This loads another smart tool, specifically designed to read emotions from faces.\n","# We set 'mtcnn=False' because we're already finding faces with our 'face_cascade'.\n","# This tells FER, \"Don't bother finding faces yourself; I'll give them to you!\"\n","emotion_detector = FER(mtcnn=False)\n","\n","# Line 3: Mark the starting time\n","# We're just setting a stopwatch here to see how long our entire process takes.\n","start = time.time()\n","\n","# --- 1. Face Detection using OpenCV Haar Cascade ---\n","# Line 4: Convert the image to grayscale\n","# Imagine taking a color photo and making it black and white. Why?\n","# Because our 'face_cascade' (the face-finding map) works much faster and more efficiently\n","# when it only has to worry about shades of gray, not millions of colors.\n","gray_image = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n","\n","# Line 5: Detect faces! This is the core face-finding step.\n","# We're giving our grayscale image to the 'face_cascade' and asking it to find all faces.\n","# The numbers (1.1, 5, (30, 30)) are like sensitivity settings:\n","#   - 1.1: How much to shrink the image each time it looks for faces (smaller means more thorough, but slower).\n","#   - 5: How many times a potential face region needs to be seen to be counted as a real face (helps avoid false alarms).\n","#   - (30, 30): The smallest face size (in pixels) we're willing to detect. Anything smaller is ignored.\n","# 'faces' will become a list of rectangles, each one describing a face's position (x, y, width, height).\n","faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n","\n","# --- Setup Plot ---\n","# Line 6: Prepare a place to draw our image and the results\n","# This creates a blank canvas (called a 'figure') and a drawing area ('axes') for our picture.\n","# 'figsize' sets the size of this canvas, making sure our image isn't tiny or huge.\n","fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n","# Line 7: Display the original color image on our drawing area\n","# We're putting our actual picture onto the canvas. 'aspect='auto'' helps it fit nicely.\n","ax.imshow(img_rgb, aspect='auto')\n","\n","print(\"\\n--- Detected Faces & Emotions ---\")\n","# Line 8: Keep a count of how many emotions we successfully detect\n","detected_emotions_count = 0\n","\n","# Line 9: Set a padding for extracting face images\n","# When we cut out a face to analyze its emotion, we want to grab a little extra space\n","# around it (like a border) so the emotion detector has more context. '20' pixels in this case.\n","padding_boundary = 20\n","\n","# Line 10: Loop through each detected face\n","# Now, we're going through each 'face' that our 'face_cascade' found, one by one.\n","# For each face, 'x', 'y', 'w', and 'h' tell us its top-left corner (x,y) and its width (w) and height (h).\n","for (x, y, w, h) in faces:\n","    # Line 11: Convert OpenCV coordinates to (top, right, bottom, left)\n","    # The 'face_recognition' library often uses this format, and it's also clear for drawing.\n","    # 'y' is 'top', 'x + w' is 'right', 'y + h' is 'bottom', and 'x' is 'left'.\n","    top, right, bottom, left = y, x + w, y + h, x\n","\n","    print(f\"  Face found at X: {x}, Y: {y}, Width: {w}, Height: {h}\")\n","\n","    # Line 12: Draw a red rectangle around the face\n","    # We're creating a rectangle object using the (left, top) corner, width, and height.\n","    # 'linewidth' is how thick the line is, 'edgecolor='r'' makes it red, and 'facecolor='none'' means it's just an outline.\n","    rect = patches.Rectangle((left, top), (right - left), (bottom - top),\n","                             linewidth=3, edgecolor='r', facecolor='none')\n","    # Line 13: Add the rectangle to our drawing area ('ax')\n","    ax.add_patch(rect)\n","\n","    # --- 2. Extract Face Image for Emotion Detection ---\n","    # Line 14: Calculate the starting Y-coordinate for slicing the face image\n","    # We take 'y' (top of the face) and subtract 'padding_boundary'.\n","    # 'max(0, ...)' makes sure we don't go below 0 (the top edge of the image).\n","    y_start = max(0, y - padding_boundary)\n","    # Line 15: Calculate the ending Y-coordinate for slicing the face image\n","    # We take 'y + h' (bottom of the face) and add 'padding_boundary'.\n","    # 'min(img_bgr.shape[0], ...)' makes sure we don't go beyond the image's total height.\n","    y_end = min(img_bgr.shape[0], y + h + padding_boundary)\n","    # Line 16: Calculate the starting X-coordinate for slicing the face image\n","    # Similar to y_start, but for the left edge.\n","    x_start = max(0, x - padding_boundary)\n","    # Line 17: Calculate the ending X-coordinate for slicing the face image\n","    # Similar to y_end, but for the right edge.\n","    x_end = min(img_bgr.shape[1], x + w + padding_boundary)\n","\n","    # Line 18: Slice the image to get just the face (with padding)\n","    # This creates a smaller image containing only the detected face region.\n","    # We're using 'img_bgr' here because it's the original image loaded by OpenCV.\n","    face_image_for_emotion = img_bgr[y_start:y_end, x_start:x_end]\n","\n","    # --- 3. Perform Emotion Detection ---\n","    # Line 19: Check if the sliced face image is not empty\n","    # Sometimes, if a face is at the very edge of the image, the slicing might result in an empty area.\n","    # This check prevents errors if that happens.\n","    if face_image_for_emotion.size > 0:\n","        # Line 20: Convert the sliced face image to RGB\n","        # Our 'fer' emotion detector often works best with RGB images, so we convert it.\n","        face_image_rgb = cv2.cvtColor(face_image_for_emotion, cv2.COLOR_BGR2RGB)\n","\n","        # Line 21: Detect emotions in this extracted face image\n","        # We give the small RGB face image to our 'emotion_detector'.\n","        # It processes it and gives us back information about the emotions it found.\n","        # This 'emotions_data' will be a list of dictionaries.\n","        emotions_data = emotion_detector.detect_emotions(face_image_rgb)\n","\n","        # Line 22: Check if any emotions were actually detected for this face\n","        # If 'emotions_data' is not empty, it means FER successfully found emotions.\n","        if emotions_data:\n","            # Line 23: Get the emotion scores for the first face detected in the slice\n","            # If FER found multiple faces in our small slice (unlikely but possible),\n","            # 'emotions_data[0]' gives us the first one's emotion scores (e.g., 'happy': 0.9, 'sad': 0.1).\n","            emotion_scores = emotions_data[0]['emotions']\n","            # Line 24: Find the emotion with the highest score\n","            # 'max(...)' finds the item with the biggest number (the highest score).\n","            # 'operator.itemgetter(1)' tells it to look at the *second* part of each item (the score itself).\n","            # '[0]' then picks out the emotion name (like 'happy').\n","            emotion = max(emotion_scores.items(), key=operator.itemgetter(1))[0]\n","            # Line 25: Increment our counter for detected emotions\n","            detected_emotions_count += 1\n","\n","            # Line 26: Plot the emotion text above the face\n","            # 'plt.text' draws text on our image.\n","            #   - 'x, y - 10': Places the text slightly above the top-left corner of the face.\n","            #   - 'fontsize', 'color': Self-explanatory.\n","            #   - 'bbox': Creates a nice colored box behind the text, making it easier to read.\n","            plt.text(x, y - 10, emotion, fontsize=12, color='blue',\n","                     bbox=dict(facecolor='yellow', edgecolor='blue', boxstyle='round,pad=0.3'))\n","        else:\n","            # Line 27: If no emotion was confidently detected, print a message and draw \"Unknown\"\n","            print(f\"  No emotion detected for face at X:{x}, Y:{y}.\")\n","            plt.text(x, y - 10, \"Unknown\", fontsize=10, color='gray',\n","                     bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.2'))\n","    else:\n","        # Line 28: If the sliced face image was empty, print a warning\n","        print(f\"  Warning: Sliced face image at X:{x}, Y:{y} was empty.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yOL4Q_ui8RC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7ajQ3lsi8VN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SS-tFnpri8ZP"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}